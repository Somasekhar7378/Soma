EXPERIMENT 9

from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import DecisionBoundaryDisplay

# Load iris dataset
iris = load_iris()

# Parameters
n_classes = 3
plot_colors = "ryb"
plot_step = 0.02

# Create a figure with 6 subplots
fig, axs = plt.subplots(2, 3, figsize=(12, 8))

# Loop over each pair of features
for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):
    # We only take the two corresponding features
    X = iris.data[:, pair]
    y = iris.target

    # Train a decision tree classifier
    clf = DecisionTreeClassifier().fit(X, y)

    # Plot the decision boundary
    ax = axs[pairidx // 3, pairidx % 3]
    disp = DecisionBoundaryDisplay.from_estimator(clf, X,
                                                  cmap=plt.cm.RdYlBu,
                                                  response_method="predict",
                                                  ax=ax,
                                                  xlabel=iris.feature_names[pair[0]],
                                                  ylabel=iris.feature_names[pair[1]],
                                                 )

    # Plot the training points
    for i, color in zip(range(n_classes), plot_colors):
        idx = np.where(y == i)
        ax.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.RdYlBu, edgecolor="black", s=15)

    ax.set_title(f"Features {pair[0]} and {pair[1]}")

plt.suptitle("Decision surface of decision trees trained on pairs of features")
plt.legend(loc="lower right", borderpad=0, handletextpad=0)
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
plt.show()

# Plot the decision tree trained on all features
plt.figure()
clf = DecisionTreeClassifier().fit(iris.data, iris.target)
plot_tree(clf, filled=True)
plt.title("Decision tree trained on all the iris features")
plt.show()